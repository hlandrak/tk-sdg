{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "from spacy.lang.nb import Norwegian\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfToText(fileName):\n",
    "    all_text = \"\"\n",
    "    with pdfplumber.open(fileName) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            all_text += page.extract_text()\n",
    "    # print(all_text)\n",
    "    all_text = re.sub('\\s', ' ', all_text)\n",
    "    # print(all_text)\n",
    "    return all_text\n",
    "\n",
    "regPlan = pdfToText('regional-planstrategi-2016-2010.pdf')\n",
    "kirkepol = pdfToText('050200-sak-kommunens-kirkepolitikk.pdf')\n",
    "smabathavn = pdfToText('kommunedelplan-for-smabathavner-2007-2017.pdf')\n",
    "kultur = pdfToText('strategiplan-kultur-web.pdf')\n",
    "havbruk = pdfToText(\"havbruk.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txtToStr(filename):\n",
    "    f = open(filename, 'r')\n",
    "    textLines = f.readlines()\n",
    "    text = \"\"\n",
    "    for line in textLines:\n",
    "        text += line\n",
    "    text = re.sub('\\s', ' ', text)\n",
    "    return text\n",
    "\n",
    "brautTxt = txtToStr(\"braut.txt\")\n",
    "byplanWiki = txtToStr(\"byplanleggingWiki.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "No need to run if \"sdg#.txt\" in \"sdgs\" folder is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping imports and inits\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping sdgs from FN.\n",
    "sdgs = []\n",
    "driver.get(\"https://www.fn.no/om-fn/fns-baerekraftsmaal\")\n",
    "soup = BS(driver.page_source, features=\"html.parser\")\n",
    "title_cards = soup.find_all(class_=\"header_gols_content_item\")\n",
    "for card in title_cards:\n",
    "    a = card.find(\"a\", href=True)\n",
    "    sdgs.append(a[\"href\"])\n",
    "print(sdgs)\n",
    "i = 1\n",
    "for sdg in sdgs:\n",
    "    driver.get(f\"https://www.fn.no/{sdg}\")\n",
    "    soup = BS(driver.page_source, features=\"html.parser\")\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    f = open(f'sdgs/sdg{i}.txt', 'w')\n",
    "    for par in paragraphs:\n",
    "        line = par.text.strip()\n",
    "        line = line.replace('...', '')\n",
    "        line = line.replace('&aelig;', 'æ')\n",
    "        if len(line) > 0:\n",
    "            if not line[-1] in \".?!:)\":\n",
    "                line += '.'\n",
    "            f.write(f'{line}\\n')\n",
    "    f.close()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarityText(mainStr, searchStr):\n",
    "    \"\"\"\n",
    "    Return spacy similatiry based on vector in nb_core_news_lg.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"nb_core_news_lg\")\n",
    "\n",
    "    mainDoc = nlp(mainStr)\n",
    "    searchDoc = nlp(searchStr)\n",
    "\n",
    "    mainTokenized = nlp(' '.join([str(token.lemma_) for token in mainDoc if not token.is_stop and not token.is_punct and not token.is_space]))\n",
    "    searchTokenized = nlp(' '.join([str(token.lemma_) for token in searchDoc if not token.is_stop and not token.is_punct and not token.is_space]))\n",
    "    # print(mainTokenized)\n",
    "    # print(searchTokenized)\n",
    "    return mainTokenized.similarity(searchTokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdgSimilatiry(string):\n",
    "    \"\"\"\n",
    "    Print and return similarity of string to all SDGs.\n",
    "    \"\"\"\n",
    "    valueList = []\n",
    "    for sdg in range(17):\n",
    "        value = similarityText(string, txtToStr(f\"sdgs/sdg{sdg+1}.txt\"))\n",
    "        print(f\"SDG #{sdg+1} has this similarity to your string: {value}\")\n",
    "        valueList.append(value)\n",
    "    return valueList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdgVector = {} # Container for similarity results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addSdgVector(str, name):\n",
    "    \"\"\"\n",
    "    Add similarity results to sdgVector.\n",
    "    \"\"\"\n",
    "    sdgVector[name] = sdgSimilatiry(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addSdgVector(regPlan, \"Regional planstrategi\")\n",
    "addSdgVector(smabathavn, \"Kommunedelplan for småbåthavner\")\n",
    "addSdgVector(kultur, \"Strategiplan kultur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for documentation see: https://www.kaggle.com/satishgunjal/tutorial-text-classification-using-spacy\n",
    "import string\n",
    "nlp = spacy.load(\"nb_core_news_lg\")\n",
    "parser = Norwegian()\n",
    "punctuations = string.punctuation\n",
    "stop_words = spacy.lang.nb.stop_words.STOP_WORDS\n",
    "def spacy_tokenizer(sentence):\n",
    "    \"\"\"This function will accepts a sentence as input and processes the sentence into tokens, performing lemmatization, \n",
    "    lowercasing, removing stop words and punctuations.\"\"\"\n",
    "    \n",
    "    # Creating our token object which is used to create documents with linguistic annotations\n",
    "    mytokens = nlp(sentence)\n",
    "    \n",
    "    # lemmatizing each token and converting each token in lower case\n",
    "    # Note that spaCy uses '-PRON-' as lemma for all personal pronouns lkike me, I etc\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    \n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations]\n",
    "    # Return preprocessed list of tokens\n",
    "    return mytokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        \"\"\"Override the transform method to clean text\"\"\"\n",
    "        return [clean_text(text) for text in X]\n",
    "    \n",
    "    def fit(self, X, y= None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep= True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    \"\"\"Removing spaces and converting the text into lowercase\"\"\"\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfVector = TfidfVectorizer(tokenizer=spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating trainingdata for classifying SDGs\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(17):\n",
    "    f = open(f'sdgs/sdg{i+1}.txt')\n",
    "    for line in f:\n",
    "        line = re.sub('\\s', ' ', line)\n",
    "        X_train.append(line)\n",
    "        y_train.append(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create pipeline using Tf-idf\n",
    "pipe = Pipeline ([(\"cleaner\", predictors()),\n",
    "                 (\"vectorizer\", tfidfVector),\n",
    "                 (\"classifier\", LogisticRegression(multi_class='ovr', solver='liblinear'))])\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cleaner', <__main__.predictors object at 0x7fbfbef84b90>),\n",
       "                ('vectorizer',\n",
       "                 TfidfVectorizer(tokenizer=<function spacy_tokenizer at 0x7fbfa4f0b950>)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(multi_class='ovr', solver='liblinear'))])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [kirkepol, regPlan, kultur]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04994933 0.05503217 0.05952103 0.04759985 0.06220152 0.05147197\n",
      "  0.05746694 0.05837788 0.0536922  0.0721915  0.0751253  0.05719292\n",
      "  0.04876716 0.05467803 0.06115876 0.06515924 0.07041422]\n",
      " [0.04718606 0.05644755 0.05659871 0.05303923 0.05172827 0.0449436\n",
      "  0.05592864 0.05318267 0.07926795 0.05565592 0.06735849 0.06680581\n",
      "  0.05187003 0.05845414 0.05584103 0.05143508 0.09425682]\n",
      " [0.04920108 0.05158487 0.05555455 0.04894139 0.04489729 0.04807293\n",
      "  0.04430519 0.06399124 0.0856254  0.05818435 0.06611883 0.06009107\n",
      "  0.0442942  0.05511224 0.06531739 0.06702736 0.09168063]]\n"
     ]
    }
   ],
   "source": [
    "def tfidfModel(trainingData, testData):\n",
    "    \"\"\"\n",
    "    Return predicted probabilities for Pipeline (pipe).\n",
    "    \"\"\"\n",
    "    pipe = Pipeline ([(\"cleaner\", predictors()),\n",
    "                 (\"vectorizer\", tfidfVector),\n",
    "                 (\"classifier\", LogisticRegression(multi_class='ovr', solver='liblinear'))])\n",
    "\n",
    "    pipe.fit(trainingData[0], trainingData[1])\n",
    "\n",
    "    predicted = pipe.predict_proba(testData)\n",
    "    return predicted\n",
    "\n",
    "sdgPredictions = tfidfModel([X_train, y_train], X_test)\n",
    "print(sdgPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfModel([havbruk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikiscraper as ws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.lang(\"no\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstractList = []\n",
    "with open(\"randWiki.txt\", \"a\") as file:\n",
    "    for i in range(1000):\n",
    "        result = ws.searchBySlug(\"Spesial:Tilfeldig\")\n",
    "        try:\n",
    "            abstractList = result.getAbstract()\n",
    "            for par in abstractList:\n",
    "                if not \"[rediger | rediger kilde]\" in par:\n",
    "                    file.write(par + \"\\n\")\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cleaner', <__main__.predictors object at 0x7fbfb02f1d50>),\n",
       "                ('vectorizer',\n",
       "                 TfidfVectorizer(tokenizer=<function spacy_tokenizer at 0x7fbfa4f0b950>)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(multi_class='ovr', solver='liblinear'))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2 = []\n",
    "y_train2 = []\n",
    "for i in range(17):\n",
    "    with open(f'sdgs/sdg{i+1}.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            line = re.sub('\\s', ' ', line)\n",
    "            X_train2.append(line)\n",
    "            y_train2.append(0)\n",
    "\n",
    "with open('randWiki.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = re.sub('\\s', ' ', line)\n",
    "        X_train2.append(line)\n",
    "        y_train2.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "baerBool = tfidfModel([X_train2, y_train2], X_test)\n",
    "sdgPredictions = tfidfModel([X_train, y_train], X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84913867 0.93554681 1.01185746 0.8091974  1.05742576 0.87502345\n",
      "  0.97693799 0.99242396 0.9127674  1.22725544 1.27713014 0.97227971\n",
      "  0.82904171 0.92952645 1.03969889 1.1077071  1.19704167]\n",
      " [0.80216301 0.95960833 0.96217811 0.90166694 0.87938053 0.76404125\n",
      "  0.95078693 0.90410536 1.34755515 0.94615066 1.14509438 1.13569871\n",
      "  0.88179046 0.99372032 0.94929748 0.87439644 1.60236593]\n",
      " [0.83641834 0.87694283 0.94442731 0.83200358 0.763254   0.8172398\n",
      "  0.75318819 1.08785113 1.45563175 0.9891339  1.12402005 1.02154812\n",
      "  0.75300147 0.93690809 1.11039561 1.13946505 1.55857078]]\n"
     ]
    }
   ],
   "source": [
    "sdgPredictions = sdgPredictions*17\n",
    "print(sdgPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.32765071 0.36099237 0.39043778 0.31223888 0.40802088 0.33763868\n",
      "  0.37696367 0.38293912 0.35220265 0.47355177 0.49279654 0.37516621\n",
      "  0.31989605 0.35866934 0.40118074 0.42742256 0.46189341]\n",
      " [0.76735672 0.91797039 0.92042867 0.86254311 0.84122372 0.73088908\n",
      "  0.90953175 0.86487573 1.28908399 0.90509666 1.0954081  1.08642012\n",
      "  0.84352908 0.95060224 0.90810694 0.8364559  1.53283839]\n",
      " [0.62755589 0.65796098 0.70859388 0.62424354 0.57266144 0.61316642\n",
      "  0.56510918 0.81620326 1.09214519 0.74213676 0.84334042 0.76645681\n",
      "  0.56496908 0.70295228 0.83311815 0.85492864 1.16937927]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    sdgPredictions[i] = sdgPredictions[i]*baerBool[i][0]\n",
    "print(sdgPredictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[2, 3, 7, 9, 10, 11, 12, 14, 15, 17]\n",
      "[9, 17]\n"
     ]
    }
   ],
   "source": [
    "def sdgList(predictions, threshold):\n",
    "    sdgTrue = [] \n",
    "    for i in range(17):\n",
    "        if predictions[i] >= threshold:\n",
    "            sdgTrue.append(i+1)\n",
    "    return sdgTrue\n",
    "\n",
    "for doku in sdgPredictions:\n",
    "    print(sdgList(doku, 0.9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
