{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "from spacy.lang.nb import Norwegian\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfToText(fileName):\n",
    "    all_text = \"\"\n",
    "    with pdfplumber.open(fileName) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            all_text += page.extract_text()\n",
    "    # print(all_text)\n",
    "    all_text = re.sub('\\s', ' ', all_text)\n",
    "    # print(all_text)\n",
    "    return all_text\n",
    "\n",
    "regPlan = pdfToText('regional-planstrategi-2016-2010.pdf')\n",
    "kirkepol = pdfToText('050200-sak-kommunens-kirkepolitikk.pdf')\n",
    "smabathavn = pdfToText('kommunedelplan-for-smabathavner-2007-2017.pdf')\n",
    "kultur = pdfToText('strategiplan-kultur-web.pdf')\n",
    "havbruk = pdfToText(\"havbruk.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txtToStr(filename):\n",
    "    f = open(filename, 'r')\n",
    "    textLines = f.readlines()\n",
    "    text = \"\"\n",
    "    for line in textLines:\n",
    "        text += line\n",
    "    text = re.sub('\\s', ' ', text)\n",
    "    return text\n",
    "\n",
    "brautTxt = txtToStr(\"braut.txt\")\n",
    "byplanWiki = txtToStr(\"byplanleggingWiki.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "No need to run if \"sdg#.txt\" in \"sdgs\" folder is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping imports and inits\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping sdgs from FN.\n",
    "sdgs = []\n",
    "driver.get(\"https://www.fn.no/om-fn/fns-baerekraftsmaal\")\n",
    "soup = BS(driver.page_source, features=\"html.parser\")\n",
    "title_cards = soup.find_all(class_=\"header_gols_content_item\")\n",
    "for card in title_cards:\n",
    "    a = card.find(\"a\", href=True)\n",
    "    sdgs.append(a[\"href\"])\n",
    "print(sdgs)\n",
    "i = 1\n",
    "for sdg in sdgs:\n",
    "    driver.get(f\"https://www.fn.no/{sdg}\")\n",
    "    soup = BS(driver.page_source, features=\"html.parser\")\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    f = open(f'sdgs/sdg{i}.txt', 'w')\n",
    "    for par in paragraphs:\n",
    "        line = par.text.strip()\n",
    "        line = line.replace('...', '')\n",
    "        line = line.replace('&aelig;', 'æ')\n",
    "        if len(line) > 0:\n",
    "            if not line[-1] in \".?!:)\":\n",
    "                line += '.'\n",
    "            f.write(f'{line}\\n')\n",
    "    f.close()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarityText(mainStr, searchStr):\n",
    "    \"\"\"\n",
    "    Return spacy similatiry based on vector in nb_core_news_lg.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"nb_core_news_lg\")\n",
    "\n",
    "    mainDoc = nlp(mainStr)\n",
    "    searchDoc = nlp(searchStr)\n",
    "\n",
    "    mainTokenized = nlp(' '.join([str(token.lemma_) for token in mainDoc if not token.is_stop and not token.is_punct and not token.is_space]))\n",
    "    searchTokenized = nlp(' '.join([str(token.lemma_) for token in searchDoc if not token.is_stop and not token.is_punct and not token.is_space]))\n",
    "    # print(mainTokenized)\n",
    "    # print(searchTokenized)\n",
    "    return mainTokenized.similarity(searchTokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdgSimilatiry(string):\n",
    "    \"\"\"\n",
    "    Print and return similarity of string to all SDGs.\n",
    "    \"\"\"\n",
    "    valueList = []\n",
    "    for sdg in range(17):\n",
    "        value = similarityText(string, txtToStr(f\"sdgs/sdg{sdg+1}.txt\"))\n",
    "        print(f\"SDG #{sdg+1} has this similarity to your string: {value}\")\n",
    "        valueList.append(value)\n",
    "    return valueList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdgVector = {} # Container for similarity results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addSdgVector(str, name):\n",
    "    \"\"\"\n",
    "    Add similarity results to sdgVector.\n",
    "    \"\"\"\n",
    "    sdgVector[name] = sdgSimilatiry(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addSdgVector(regPlan, \"Regional planstrategi\")\n",
    "addSdgVector(smabathavn, \"Kommunedelplan for småbåthavner\")\n",
    "addSdgVector(kultur, \"Strategiplan kultur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for documentation see: https://www.kaggle.com/satishgunjal/tutorial-text-classification-using-spacy\n",
    "import string\n",
    "nlp = spacy.load(\"nb_core_news_lg\")\n",
    "parser = Norwegian()\n",
    "punctuations = string.punctuation\n",
    "stop_words = spacy.lang.nb.stop_words.STOP_WORDS\n",
    "def spacy_tokenizer(sentence):\n",
    "    \"\"\"This function will accepts a sentence as input and processes the sentence into tokens, performing lemmatization, \n",
    "    lowercasing, removing stop words and punctuations.\"\"\"\n",
    "    \n",
    "    # Creating our token object which is used to create documents with linguistic annotations\n",
    "    mytokens = nlp(sentence)\n",
    "    \n",
    "    # lemmatizing each token and converting each token in lower case\n",
    "    # Note that spaCy uses '-PRON-' as lemma for all personal pronouns lkike me, I etc\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    \n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations]\n",
    "    # Return preprocessed list of tokens\n",
    "    return mytokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        \"\"\"Override the transform method to clean text\"\"\"\n",
    "        return [clean_text(text) for text in X]\n",
    "    \n",
    "    def fit(self, X, y= None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep= True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    \"\"\"Removing spaces and converting the text into lowercase\"\"\"\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfVector = TfidfVectorizer(tokenizer=spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating trainingdata for classifying SDGs\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(17):\n",
    "    f = open(f'sdgs/sdg{i+1}.txt')\n",
    "    for line in f:\n",
    "        line = re.sub('\\s', ' ', line)\n",
    "        X_train.append(line)\n",
    "        y_train.append(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
    "\n",
    "# Create pipeline using Tf-idf\n",
    "pipe = Pipeline ([(\"cleaner\", predictors()),\n",
    "                 (\"vectorizer\", tfidfVector),\n",
    "                 (\"classifier\", classifier)])\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cleaner', <__main__.predictors object at 0x7fb992a34690>),\n",
       "                ('vectorizer',\n",
       "                 TfidfVectorizer(tokenizer=<function spacy_tokenizer at 0x7fb98da884d0>)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(multi_class='ovr', solver='liblinear'))])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [kirkepol, regPlan, kultur]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05007421 0.05547551 0.05961166 0.04732276 0.06221764 0.05157153\n",
      "  0.05782691 0.05784505 0.05378199 0.07203965 0.07491142 0.05713694\n",
      "  0.04850368 0.0548395  0.06136086 0.06523781 0.07024289]\n",
      " [0.04705317 0.05632377 0.0566746  0.05294855 0.05177795 0.0450207\n",
      "  0.05609791 0.05307658 0.07936843 0.05567265 0.06739587 0.06689712\n",
      "  0.05181272 0.05858311 0.05592472 0.05140032 0.09397184]\n",
      " [0.04928288 0.05188751 0.05563631 0.04871648 0.04493027 0.04819156\n",
      "  0.04458906 0.06354457 0.08569743 0.058139   0.06599206 0.06008801\n",
      "  0.04411382 0.05527874 0.0655141  0.06703287 0.09136534]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.05007421, 0.05547551, 0.05961166, 0.04732276, 0.06221764,\n",
       "        0.05157153, 0.05782691, 0.05784505, 0.05378199, 0.07203965,\n",
       "        0.07491142, 0.05713694, 0.04850368, 0.0548395 , 0.06136086,\n",
       "        0.06523781, 0.07024289],\n",
       "       [0.04705317, 0.05632377, 0.0566746 , 0.05294855, 0.05177795,\n",
       "        0.0450207 , 0.05609791, 0.05307658, 0.07936843, 0.05567265,\n",
       "        0.06739587, 0.06689712, 0.05181272, 0.05858311, 0.05592472,\n",
       "        0.05140032, 0.09397184],\n",
       "       [0.04928288, 0.05188751, 0.05563631, 0.04871648, 0.04493027,\n",
       "        0.04819156, 0.04458906, 0.06354457, 0.08569743, 0.058139  ,\n",
       "        0.06599206, 0.06008801, 0.04411382, 0.05527874, 0.0655141 ,\n",
       "        0.06703287, 0.09136534]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tfidfModel(testData):\n",
    "    \"\"\"\n",
    "    Return predicted probabilities for Pipeline (pipe).\n",
    "    \"\"\"\n",
    "    predicted = pipe.predict_proba(testData)\n",
    "    return predicted\n",
    "\n",
    "tfidfModel(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03982087 0.05026226 0.06192469 0.05234734 0.05295878 0.05419719\n",
      "  0.07524212 0.03971253 0.05953944 0.06374593 0.06831475 0.04775197\n",
      "  0.05092737 0.06188385 0.06360098 0.07821822 0.07955172]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03982087, 0.05026226, 0.06192469, 0.05234734, 0.05295878,\n",
       "        0.05419719, 0.07524212, 0.03971253, 0.05953944, 0.06374593,\n",
       "        0.06831475, 0.04775197, 0.05092737, 0.06188385, 0.06360098,\n",
       "        0.07821822, 0.07955172]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfModel([byplanWiki])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
